## Schema registry and why it is useful

It is pretty obvious why schemas are useful when multiple **clients need to collaborate on the same data**. They all
need to know the metadata (which fields are available and what are their types). Moreover, data storage and processing
is more efficient, because we don't need to send the schema with each message and numbers can be stored in their more
efficient binary representation.

It is less obvious why we may need a central schema registry. Why would you want to add this complexity to your
architecture? Why not simply agree on a message schema, put it in shared library and live with that? Distributing
schemas is easy when we have few clients that we control, but it becomes challenging when we have lots of clients and
some of them are by third party. In addition to distribution, we also need to evolve these schema safely in a backwards
and forwards compatible way, to allow new clients to read old data and old clients to read new data.

![](images/serdes.png)

[Red Hat Service Registry](https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry)
is based on [Apicurio Registry](https://www.apicur.io/registry), which is a schema registry for REST APIs (OpenAPI) and
message schemas (AsyncAPI). It supports pluggable storage (in-memory, Kafka, PostgreSQL), schema versioning, schema
validation, provides **Java serializers/deserializers** (SerDes) for Avro, Protobuf and JSON Schemaa and a **Maven
plugin** to automatically register artifacts at build time. There are also a REST API and a web console, that can be
used to do CRUD operations on schemas. When migrating from Confluent registry, it is possible to enable the API
translation layer and use a tool called `exportConfluent` to import existing schemas.

A registered schema artifact is uniquely identified by the tuple `(groupId, artifactId, version)`. The `groupId` is
provided by the user and it is just a way to logically group artifacts. By default, the `artifactId` is equal to the
topic name plus `-key` or `-value` suffix, depending on whether the serializer was used for the message key or value.
The `globalId` and `contentId` are generated by the server. The `globalId` is the unique id of an artifact version,
while the `contentId` is the unique id of the artifact content (different artifacts containing the same schema have the
same content id).

The serializer exchanges the `artifactId` for a server generated `globalId`, which is added as record header or as
payload prefix, depending on the producer configuration. The deserializer fetches the right schema version using
the `globalId`. If required, you can configure to fetch by `contentId` (Confluent default). The `CHECK_PERIOD_MS` is the
time after which a cached artifact is auto evicted and needs to be fetched again on the next record.

### Example: schema registry in action

[Deploy Streams operator and Kafka cluster](/sessions/001). Then, we need to deploy the Service Registry operator. When
the operators are ready, we can deploy the Service Registry instance with PostgreSQL as storage system.

```sh
$ kubectl create -f sessions/003/sub.yaml
subscription.operators.coreos.com/my-svcreg created
Error from server (AlreadyExists): error when creating "sessions/003/sub.yaml": operatorgroups.operators.coreos.com "local-operators" already exists

$ kubectl get po -n openshift-operators
NAME                                                     READY   STATUS    RESTARTS   AGE
amq-streams-cluster-operator-v2.1.0-8-6dfcc6449d-c2np5   1/1     Running   3          2d20h
apicurio-registry-operator-fb9ffd5cb-89z89               1/1     Running   0          46s

$ kubectl create -f sessions/003/crs
configmap/my-pgsql-init created
persistentvolumeclaim/my-pgsql-pvc created
configmap/my-pgsql-config created
configmap/my-pgsql-env created
statefulset.apps/my-pgsql-ss created
service/my-pgsql-svc created
apicurioregistry.registry.apicur.io/my-registry created

$ kubectl get po
NAME                                              READY   STATUS    RESTARTS   AGE
pod/my-cluster-entity-operator-6b68959588-698hp   3/3     Running   0          165m
pod/my-cluster-kafka-0                            1/1     Running   0          166m
pod/my-cluster-kafka-1                            1/1     Running   0          166m
pod/my-cluster-kafka-2                            1/1     Running   0          166m
pod/my-cluster-zookeeper-0                        1/1     Running   0          168m
pod/my-cluster-zookeeper-1                        1/1     Running   0          168m
pod/my-cluster-zookeeper-2                        1/1     Running   0          168m
pod/my-pgsql-ss-0                                 1/1     Running   0          8m36s
pod/my-registry-deployment-5f5fb7c786-7tj75       1/1     Running   0          53s
```

Now, we just need to tell our client application where it can find the Kafka cluster by setting the bootstrap URL and
the schema registry REST endpoint. We also need to provide the truststore location and password because we are
connecting externally.

```sh
$ export BOOTSTRAP_SERVERS=$(kubectl get k my-cluster -o yaml | yq e '.status.listeners[2].bootstrapServers') \
  && export REGISTRY_URL="http://$(kubectl get apicurioregistries my-registry -o 'jsonpath={.status.info.host}')/apis/registry/v2" \
  && kubectl get secret my-cluster-cluster-ca-cert -o "jsonpath={.data['ca\.p12']}" | base64 -d > /tmp/truststore.p12 \
  && export SSL_TRUSTSTORE_LOCATION="/tmp/truststore.p12" \
  && export SSL_TRUSTSTORE_PASSWORD=$(kubectl get secret my-cluster-cluster-ca-cert -o "jsonpath={.data['ca\.password']}" | base64 -d)

$ pushd sessions/003/kafka-avro && mvn clean compile exec:java -q && popd
~/Documents/streams-debugging/sessions/003/kafka-avro ~/Documents/streams-debugging
Producing records
Records produced
Consuming all records
Record: Hello-1663594981476
Record: Hello-1663594982041
Record: Hello-1663594982041
Record: Hello-1663594982041
Record: Hello-1663594982042
~/Documents/streams-debugging
```

[Look at the code](/sessions/003/kafka-avro) to see how the schema is registered and used. The registration happens at
build time and the Maven plugin execute the following API request for every configured schema artifact. Note that we are
using the "default" group id, but you can specify a custom name.

```sh
$ export REGISTRY_URL="http://$(kubectl get apicurioregistries my-registry -o 'jsonpath={.status.info.host}')/apis/registry/v2"
$ curl -s -X POST -H "Content-Type: application/json" \
  -H "X-Registry-ArtifactId: my-topic-value" -H "X-Registry-ArtifactType: AVRO" \
    -d @src/main/resources/greeting.avsc $REGISTRY_URL/groups/default/artifacts?ifExists=RETURN_OR_UPDATE
{
  "name": "Greeting",
  "createdBy": "",
  "createdOn": "2022-09-30T06:31:36+0000",
  "modifiedBy": "",
  "modifiedOn": "2022-09-30T06:31:36+0000",
  "id": "my-topic-value",
  "version": "1",
  "type": "AVRO",
  "globalId": 4,
  "state": "ENABLED",
  "contentId": 6
}
```

Finally, let's use the REST API to confirm that our schema was registered correctly. We can also look at the schema
content and metadata, which may be useful for debugging.

```sh
$ curl -s $REGISTRY_URL/search/artifacts | jq
{
  "artifacts": [
    {
      "id": "my-topic-value",
      "name": "Greeting",
      "createdOn": "2022-09-19T13:42:59+0000",
      "createdBy": "",
      "type": "AVRO",
      "state": "ENABLED",
      "modifiedOn": "2022-09-19T13:42:59+0000",
      "modifiedBy": ""
    }
  ],
  "count": 1
}

$ curl -s $REGISTRY_URL/groups/default/artifacts/my-topic-value | jq
{
  "type": "record",
  "name": "Greeting",
  "fields": [
    {
      "name": "Message",
      "type": "string"
    },
    {
      "name": "Time",
      "type": "long"
    }
  ]
}

$ curl -s $REGISTRY_URL/groups/default/artifacts/my-topic-value/meta | jq
{
  "name": "Greeting",
  "createdBy": "",
  "createdOn": "2022-09-19T13:42:59+0000",
  "modifiedBy": "",
  "modifiedOn": "2022-09-19T13:42:59+0000",
  "id": "my-topic-value",
  "version": "1",
  "type": "AVRO",
  "globalId": 1,
  "state": "ENABLED",
  "contentId": 1
}
```
